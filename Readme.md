# ğŸ§  Sistema de Reconocimiento de Lenguaje de SeÃ±as

Reconoce **lenguaje de seÃ±as en tiempo real** usando **MediaPipe Holistic** y un **clasificador LSTM bidireccional**.  
El sistema es **modular, extensible y adecuado para investigaciÃ³n, docencia y prototipado avanzado** en visiÃ³n por computador.

---

## ğŸš€ CaracterÃ­sticas

- ğŸ¯ ExtracciÃ³n en tiempo real de **543 landmarks** (manos, rostro y pose corporal) usando **MediaPipe Holistic**.  
- ğŸ” **Clasificador LSTM bidireccional** para secuencias temporales de keypoints.  
- ğŸ§© **Pipeline modular** para recolecciÃ³n de datos, preprocesamiento, entrenamiento y evaluaciÃ³n.  
- ğŸ“¸ **Inferencia interactiva desde webcam** con visualizaciÃ³n de confianza y predicciÃ³n.  
- ğŸ§  **Data augmentation** y **normalizaciÃ³n avanzada** para mejorar la robustez del modelo.  
- ğŸ“Š MÃ©tricas: *Top-k accuracy*, matriz de confusiÃ³n y reporte por clase.  
- âš™ï¸ Arquitectura **escalable y fÃ¡cil de mantener**, con soporte a datasets externos (WLASL, PHOENIX-2014T, propios).

---

## ğŸ§° InstalaciÃ³n

### 1. Crea un entorno virtual

python -m venv venv
source venv/bin/activate        # En Windows: venv\Scripts\activate

### 2. Instala las dependencias

pip install -r requirements.txt
âš¡ Uso rÃ¡pido
# 1. Recolectar datos

python scripts/collect_data.py
Ingresa las seÃ±as separadas por coma (ejemplo: hola,adios,gracias)

Define el nÃºmero de muestras por seÃ±a.

Sigue las instrucciones en pantalla.

# 2. Entrenar el modelo
python scripts/train_model.py
El modelo entrenado se guarda en:
models/best_model.pth
# 3. Inferencia en tiempo real
python scripts/run_inference.py
Pulsa q para salir.
Pulsa r para reiniciar el buffer de predicciÃ³n.

## ğŸ—‚ï¸ Estructura del proyecto
sign_language_translator/
â”œâ”€â”€ data/           # Datos crudos y secuencias procesadas
â”œâ”€â”€ src/            # CÃ³digo fuente modular
â”‚   â”œâ”€â”€ data/       # Captura, datasets y procesamiento
â”‚   â”œâ”€â”€ models/     # Modelos LSTM, Transformer, etc.
â”‚   â”œâ”€â”€ training/   # Entrenamiento y mÃ©tricas
â”‚   â”œâ”€â”€ inference/  # Inferencia en tiempo real
â”‚   â””â”€â”€ utils/      # ConfiguraciÃ³n y utilidades
â”œâ”€â”€ scripts/        # Scripts CLI
â””â”€â”€ notebooks/      # Notebooks para exploraciÃ³n y anÃ¡lisis
## âš™ï¸ ConfiguraciÃ³n
Edita el archivo:
src/utils/config.py
Puedes cambiar:

HiperparÃ¡metros del modelo (capas, tamaÃ±o de secuencia, etc.)

Rutas de datos, salidas y modelos

ParÃ¡metros de entrenamiento y tolerancia de inferencia

## ğŸ”§ ExtensiÃ³n y Escalabilidad
ğŸ”¹ Agregar Transformer con CTC
Implementa src/models/transformer.py como arquitectura tipo PHOENIX-2014T para traducciÃ³n continua de video â†’ glosas â†’ texto.

ğŸ”¹ Soporte a LSC (Lengua de SeÃ±as Colombiana)
Recolecta tu propio corpus y anÃ³talo con ELAN.

Ajusta la lista de glosas y vocabulario en la configuraciÃ³n del pipeline.

## ğŸ“š Ejemplo de sistemas soportados
ExtracciÃ³n robusta de landmarks con MediaPipe Holistic (documentaciÃ³n oficial)

Entrenamiento en tu propio lenguaje de seÃ±as o datasets pÃºblicos:
WLASL
PHOENIX-2014T
IntegraciÃ³n directa con notebooks para prototipado y visualizaciÃ³n avanzada.

## ğŸ”— Referencias
ğŸ“˜ MediaPipe Holistic
ğŸ“„ Sign Language Transformers (Paper)
ğŸ“‚ WLASL Dataset
ğŸ“‚ PHOENIX-2014T Dataset (traducciÃ³n continua)

## ğŸ§© Ejemplos comunitarios: busca proyectos SignLanguageRecognition en GitHub.
ğŸ‘¥ CrÃ©ditos
Proyecto desarrollado con fines acadÃ©micos, de docencia e investigaciÃ³n en:
VisiÃ³n por computador
Aprendizaje profundo
Accesibilidad e inclusiÃ³n tecnolÃ³gica
